{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayaselimm/Airbnb-price-category-prediction/blob/main/Assignment_1_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-SOCfgJtHJw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e01f3e57-7443-4189-a528-4823a296c54f"
      },
      "source": [
        "import numpy as np\n",
        "from enum import IntEnum\n",
        "from copy import deepcopy\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('seaborn-notebook')\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "import matplotlib.colors as mcolors"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-b53f5efa330b>:5: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-notebook')\n",
            "<ipython-input-1-b53f5efa330b>:6: MatplotlibDeprecationWarning: The seaborn styles shipped by Matplotlib are deprecated since 3.6, as they no longer correspond to the styles shipped by seaborn. However, they will remain available as 'seaborn-v0_8-<style>'. Alternatively, directly use the seaborn API instead.\n",
            "  plt.style.use('seaborn-whitegrid')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1) Linear Solver Method**"
      ],
      "metadata": {
        "id": "yQq2YuOsy54c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Action(IntEnum):\n",
        "    \"\"\"The class Action represents the set of possible actions in gridworld.\"\"\"\n",
        "    up = 0\n",
        "    right = 1\n",
        "    down = 2\n",
        "    left = 3\n",
        "\n",
        "# String representation for each action saved numerically (easier for human interpertation)\n",
        "action_to_str = {\n",
        "    Action.up: \"up\",\n",
        "    Action.right: \"right\",\n",
        "    Action.down: \"down\",\n",
        "    Action.left: \"left\",\n",
        "}\n",
        "\n",
        "# How locations should be updated given any of the 4 actions\n",
        "action_to_offset = {\n",
        "    Action.up: (-1, 0),\n",
        "    Action.right: (0, 1),\n",
        "    Action.down: (1, 0),\n",
        "    Action.left: (0, -1),\n",
        "}\n"
      ],
      "metadata": {
        "id": "ZXQ368niMllC"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GridWorld:\n",
        "\n",
        "    def __init__(self, height, width, goal, goal_value=5.0, danger=[], danger_value=-5.0, blocked=[], noise=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the GridWorld environment.\n",
        "        Creates a gridworld like MDP\n",
        "         - height (int): Number of rows\n",
        "         - width (int): Number of columns\n",
        "         - goal (int): Index number of goal cell\n",
        "         - goal_value (float): Reward given for goal cell\n",
        "         - danger (list of int): Indices of cells marked as danger\n",
        "         - danger_value (float): Reward given for danger cell\n",
        "         - blocked (list of int): Indices of cells marked as blocked (can't enter)\n",
        "         - noise (float): probability of resulting state not being what was expected\n",
        "        \"\"\"\n",
        "        self._width = width\n",
        "        self._height = height\n",
        "        self._grid_values = [0 for _ in range(height * width)]\n",
        "        self._goal_value = goal_value\n",
        "        self._danger_value = danger_value\n",
        "        self._goal_cell = goal\n",
        "        self._danger_cells = danger\n",
        "        self._blocked_cells = blocked\n",
        "        self._noise = noise  # Noise level in the environment.\n",
        "        assert noise >= 0 and noise < 1  # Ensure valid noise value.\n",
        "        self.create_next_values()  # Initialize the next state values.\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the state values to their initial state.\n",
        "        \"\"\"\n",
        "        self._grid_values = [0 for _ in range(self._height * self._width)]\n",
        "        self.create_next_values()\n",
        "\n",
        "    def _inbounds(self, state):\n",
        "        \"\"\"\n",
        "        Check if a state index is within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return state >= 0 and state < self._width * self._height\n",
        "\n",
        "    def _inbounds_rc(self, state_r, state_c):\n",
        "        \"\"\"\n",
        "        Check if row and column indices are within the grid boundaries.\n",
        "        \"\"\"\n",
        "        return state_r >= 0 and state_r < self._height and state_c >= 0 and state_c < self._width\n",
        "\n",
        "    def _state_to_rc(self, state):\n",
        "        \"\"\"\n",
        "        Convert a state index to row and column indices.\n",
        "        \"\"\"\n",
        "        return state // self._width, state % self._width\n",
        "\n",
        "    def _state_from_action(self, state, action):\n",
        "        \"\"\"\n",
        "        Gets the state as a result of applying the given action\n",
        "        \"\"\"\n",
        "        # Convert the current state to row and column indices\n",
        "        row, col = self._state_to_rc(state)\n",
        "        # Determine the row and column offsets from the action\n",
        "        offset_row, offset_col = action_to_offset[action]\n",
        "        # Calculate the new row and column after applying the action\n",
        "        next_row, next_col = row + offset_row, col + offset_col\n",
        "        # Convert the new row and column back into a state representation\n",
        "        next_state = next_row * self._width + next_col\n",
        "        # Check if the new position is within grid boundaries and not blocked\n",
        "        if self._inbounds_rc(next_row, next_col):\n",
        "            if next_state not in self._blocked_cells:\n",
        "                return next_state\n",
        "        return state\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        \"\"\"\n",
        "        Returns true if a state is terminal (goal, or danger)\n",
        "        \"\"\"\n",
        "        return state == self._goal_cell or state in self._danger_cells\n",
        "\n",
        "    def get_states(self):\n",
        "        \"\"\"\n",
        "        Gets all non-terminal states in the environment\n",
        "        \"\"\"\n",
        "        non_blocked_states = []\n",
        "        for s in range(self._height * self._width):\n",
        "          if s not in self._blocked_cells:\n",
        "            non_blocked_states.append(s)\n",
        "        return non_blocked_states\n",
        "\n",
        "    def get_actions(self, state):\n",
        "        \"\"\"\n",
        "        Returns a list of valid actions given the current state\n",
        "        \"\"\"\n",
        "        if self.is_terminal(state):\n",
        "            return []\n",
        "        return list(Action)\n",
        "\n",
        "    def get_reward(self, state):\n",
        "        \"\"\"\n",
        "        Get the reward for being in the current state\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        # Reward is non-zero for danger or goal\n",
        "        if state == self._goal_cell:\n",
        "            return self._goal_value\n",
        "        elif state in self._danger_cells:\n",
        "            return self._danger_value\n",
        "        return 0.0\n",
        "\n",
        "    def get_transitions(self, state, action):\n",
        "        \"\"\"\n",
        "        Get a list of transitions as a result of attempting the action in the current state\n",
        "        Each item in the list is a dictionary, containing the probability of reaching that state and the state itself\n",
        "        \"\"\"\n",
        "        # If the current state is terminal, the only transition is to remain in the same state with probability 1.\n",
        "        if self.is_terminal(state):\n",
        "            return [{'prob': 1.0, 'state': state}]\n",
        "\n",
        "        transitions = []\n",
        "        # Calculate the state that directly results from the intended action.\n",
        "        intended_state = self._state_from_action(state, action)\n",
        "        # The most likely outcome is moving to the intended state, affected by 1 minus the noise level.\n",
        "        transitions.append({'prob': 1 - self._noise, 'state': intended_state})\n",
        "\n",
        "        # If there is noise in the system, calculate potential unintended transitions.\n",
        "        if self._noise > 0:\n",
        "            # Identify other possible actions from the current state.\n",
        "            alternative_actions = [a for a in self.get_actions(state) if a != action]\n",
        "            if alternative_actions:  # Ensure there are alternative actions to consider.\n",
        "                # Calculate the probability of each unintended action.\n",
        "                noise_probability = self._noise / len(alternative_actions)\n",
        "                for alt_action in alternative_actions:\n",
        "                    # Calculate the state resulting from each unintended action.\n",
        "                    unintended_state = self._state_from_action(state, alt_action)\n",
        "                    # Append the unintended state with its associated noise-induced probability.\n",
        "                    transitions.append({'prob': noise_probability, 'state': unintended_state})\n",
        "\n",
        "        return transitions\n",
        "\n",
        "    def get_value(self, state):\n",
        "        \"\"\"\n",
        "        Get the current value of the state\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        return self._grid_values[state]\n",
        "\n",
        "    def create_next_values(self):\n",
        "        \"\"\"\n",
        "        Creates a temporary storage for state value updating\n",
        "        If this is not used, then asynchronous updating may result in unexpected results\n",
        "        To use properly, run this at the start of each iteration\n",
        "        \"\"\"\n",
        "        self._next_values = list(self._grid_values)\n",
        "\n",
        "    def set_next_values(self):\n",
        "        \"\"\"\n",
        "        Set the state values from the temporary copied values\n",
        "        To use properly, run this at the end of each iteration\n",
        "        \"\"\"\n",
        "        self._grid_values = self._next_values.copy()\n",
        "\n",
        "    def set_value(self, state, value):\n",
        "        \"\"\"\n",
        "        Set the value of the state into the temporary copy\n",
        "        This value will not update into main storage until self.set_next_values() is called.\n",
        "        \"\"\"\n",
        "        assert self._inbounds(state)\n",
        "        self._next_values[state] = value\n",
        "\n",
        "    def _state_to_index(self, states):\n",
        "        \"\"\"Initiate a dictionary maps a state to its index\"\"\"\n",
        "        state_to_index_map = {}\n",
        "        for idx, state in enumerate(states):\n",
        "              state_to_index_map[state] = idx\n",
        "        return state_to_index_map\n",
        "\n",
        "    def solve_linear_system(self, discount_factor=1.0):\n",
        "        \"\"\"\n",
        "        Solve the gridworld using a system of linear equations.\n",
        "        :param discount_factor: The discount factor for future rewards.\n",
        "        \"\"\"\n",
        "        # To solve the linear system of equations\n",
        "        #   find the values of metrices A and B\n",
        "\n",
        "        # Calculate the total number of states based on grid dimensions.\n",
        "        num_states = self._width * self._height\n",
        "        # Initialize the matrix A for coefficients and vector b for constants.\n",
        "        A = np.zeros((num_states, num_states))\n",
        "        b = np.zeros(num_states)\n",
        "\n",
        "        # Populate the matrix A and vector b based on the transition probabilities and rewards.\n",
        "        for state in range(num_states):\n",
        "            if state in self._blocked_cells or self.is_terminal(state):\n",
        "                # For terminal or blocked states, the value is fixed to the immediate reward.\n",
        "                A[state, state] = 1\n",
        "                b[state] = self.get_reward(state)\n",
        "            else:\n",
        "                # For non-terminal states, set up the transitions based on the Bellman equation.\n",
        "                A[state, state] = 1\n",
        "                actions = self.get_actions(state)\n",
        "                transition_probability = 1 / len(actions)\n",
        "\n",
        "                for action in actions:\n",
        "                    next_state = self._state_from_action(state, action)\n",
        "                    reward = self.get_reward(next_state)\n",
        "                    A[state, next_state] -= discount_factor * transition_probability\n",
        "                    b[state] += reward * transition_probability\n",
        "\n",
        "        # Solve the system of linear equations A * V = b to find the value function V.\n",
        "        V = np.linalg.solve(A, b)\n",
        "\n",
        "        # Round the solution for better readability and set the values in the grid.\n",
        "        V_rounded = np.round(V, 2)\n",
        "        for state in range(num_states):\n",
        "            self.set_value(state, V_rounded[state])\n",
        "\n",
        "        return V_rounded.reshape((self._height, self._width))\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"\n",
        "        Pretty print the state values\n",
        "        \"\"\"\n",
        "        out_str = \"\"\n",
        "        for r in range(self._height):\n",
        "            for c in range(self._width):\n",
        "                cell = r * self._width + c\n",
        "                if cell in self._blocked_cells:\n",
        "                    out_str += \"{:>6}\".format(\"----\")\n",
        "                elif cell == self._goal_cell:\n",
        "                    out_str += \"{:>6}\".format(\"GOAL\")\n",
        "                elif cell in self._danger_cells:\n",
        "                    out_str += \"{:>6.2f}\".format(self._danger_value)\n",
        "                else:\n",
        "                    out_str += \"{:>6.2f}\".format(self._grid_values[cell])\n",
        "                out_str += \" \"\n",
        "            out_str += \"\\n\"\n",
        "        return out_str"
      ],
      "metadata": {
        "id": "FNFooVyFMZjA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize your GridWorld\n",
        "simple_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "\n",
        "# Solve the linear system\n",
        "print(\"Solving with linear solver: (discount_factor=1)\")\n",
        "values_grid = simple_gw.solve_linear_system()\n",
        "print(values_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiylKX0fMtZ5",
        "outputId": "f4a7bee7-9aab-4a13-a128-cd3396bd8f0e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with linear solver: (discount_factor=1)\n",
            "[[-9.97 -9.99 -5.   -3.33  0.  ]\n",
            " [-9.96  0.    0.    0.    3.33]\n",
            " [-9.94  0.    0.    0.    5.  ]\n",
            " [-9.93 -9.88 -9.71 -5.   -1.6 ]\n",
            " [-9.96 -5.   -9.24 -8.02 -4.81]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Solving with linear solver: (discount_factor=0.95)\")\n",
        "values_grid = simple_gw.solve_linear_system(discount_factor=0.95)\n",
        "print(values_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IORA29rlWs92",
        "outputId": "96b880ba-cc16-4df7-d2dc-743d527bf2f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with linear solver: (discount_factor=0.95)\n",
            "[[-5.25 -7.02 -5.   -3.2  -0.  ]\n",
            " [-4.59  0.    0.   -0.    3.2 ]\n",
            " [-4.89  0.    0.   -0.    5.  ]\n",
            " [-6.22 -7.62 -8.   -5.   -1.1 ]\n",
            " [-7.46 -5.   -7.78 -6.72 -3.54]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Solving with linear solver: (discount_factor=0.75)\")\n",
        "values_grid = simple_gw.solve_linear_system(discount_factor=0.75)\n",
        "print(values_grid)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIZxVr0CLMmc",
        "outputId": "499ac446-0a4e-4f8a-898e-28cc11799750"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with linear solver: (discount_factor=0.75)\n",
            "[[-1.37 -3.91 -5.   -2.69  0.  ]\n",
            " [-0.66  0.    0.    0.    2.69]\n",
            " [-0.84  0.    0.    0.    5.  ]\n",
            " [-2.14 -4.29 -4.78 -5.   -0.3 ]\n",
            " [-4.14 -5.   -4.74 -4.09 -1.32]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "lOsaKM_Yyt3U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2) Dynamic Programming Solver**"
      ],
      "metadata": {
        "id": "lEkSmh_Eyv4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def value_iteration(grid_world, discount_factor, tolerance=0.1):\n",
        "    \"\"\"\n",
        "    Perform value iteration to solve the gridworld.\n",
        "\n",
        "    Args:\n",
        "        grid_world : The gridworld environment.\n",
        "        discount_factor: The discount factor for future rewards.\n",
        "        tolerance : The convergence threshold for value updates.\n",
        "\n",
        "    Return The number of iterations taken to converge.\n",
        "    \"\"\"\n",
        "    grid_world.reset()\n",
        "    iterations = 0\n",
        "\n",
        "    while True:\n",
        "        iterations += 1\n",
        "        max_delta = 0  # Track the maximum change in value\n",
        "\n",
        "        grid_world.create_next_values()\n",
        "\n",
        "        for state in grid_world.get_states():\n",
        "            if grid_world.is_terminal(state):\n",
        "                continue  # Skip terminal states as their value is fixed\n",
        "\n",
        "            action_values = []\n",
        "\n",
        "            for action in grid_world.get_actions(state):\n",
        "                expected_value = 0\n",
        "\n",
        "                for transition in grid_world.get_transitions(state, action):\n",
        "                    next_state = transition['state']\n",
        "                    probability = transition['prob']\n",
        "                    reward = grid_world.get_reward(next_state)\n",
        "                    expected_value += probability * (reward + discount_factor * grid_world.get_value(next_state))\n",
        "\n",
        "                action_values.append(expected_value)\n",
        "\n",
        "            # Select the maximum value among all possible actions\n",
        "            optimal_value = max(action_values)\n",
        "            max_delta = max(max_delta, abs(optimal_value - grid_world.get_value(state)))\n",
        "\n",
        "            grid_world.set_value(state, optimal_value)\n",
        "\n",
        "        grid_world.set_next_values()\n",
        "\n",
        "        if max_delta < tolerance:\n",
        "            break  # Convergence criterion met\n",
        "\n",
        "    return iterations\n",
        "\n",
        "action_symbols = {\n",
        "    Action.up: \" ↑ \",\n",
        "    Action.right: \" → \",\n",
        "    Action.down: \" ↓ \",\n",
        "    Action.left: \" ← \"\n",
        "}\n",
        "def print_policy(grid_world, discount):\n",
        "    \"\"\"\n",
        "    Print the optimal policy for the gridworld, marking the best action for each state.\n",
        "    \"\"\"\n",
        "    # Initialize the policy grid with spaces for better readability\n",
        "    policy_grid = [[\" \" for _ in range(grid_world._width)] for _ in range(grid_world._height)]\n",
        "\n",
        "    # Iterate over all states in the gridworld\n",
        "    for state in range(grid_world._height * grid_world._width):\n",
        "        row, col = grid_world._state_to_rc(state)\n",
        "\n",
        "        # Mark blocked cells with '🚫'\n",
        "        if state in grid_world._blocked_cells:\n",
        "            policy_grid[row][col] = \"🚫\"\n",
        "\n",
        "        # Mark the goal cell with '🥳'\n",
        "        elif state == grid_world._goal_cell:\n",
        "            policy_grid[row][col] = \"🥳\"\n",
        "\n",
        "        # Mark danger cells with '☠️'\n",
        "        elif state in grid_world._danger_cells:\n",
        "            policy_grid[row][col] = \"☠️\"\n",
        "\n",
        "        # For non-terminal states, compute the best action\n",
        "        elif not grid_world.is_terminal(state):\n",
        "            best_action = None\n",
        "            best_value = float('-inf')\n",
        "\n",
        "            # Evaluate each possible action from the current state\n",
        "            for action in grid_world.get_actions(state):\n",
        "                action_value = 0\n",
        "\n",
        "                # Sum the expected values for taking this action\n",
        "                for transition in grid_world.get_transitions(state, action):\n",
        "                    next_state = transition['state']\n",
        "                    prob = transition['prob']\n",
        "                    reward = grid_world.get_reward(next_state)\n",
        "                    action_value += prob * (reward + discount * grid_world.get_value(next_state))\n",
        "\n",
        "                # Update the best action if the current action has a higher value\n",
        "                if action_value > best_value:\n",
        "                    best_value = action_value\n",
        "                    best_action = action\n",
        "\n",
        "            # Map the best action to its symbol and place it in the policy grid\n",
        "            policy_grid[row][col] = action_symbols[best_action]\n",
        "\n",
        "    # Print the policy grid row by row\n",
        "    for row in policy_grid:\n",
        "        print(\" \".join(row))\n",
        "\n"
      ],
      "metadata": {
        "id": "nxmSqpsRhuHm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize your GridWorld\n",
        "simple_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.0)\n",
        "noisy_gw = GridWorld(height=5, width=5, goal=14, danger=[2, 18, 21], blocked=[6, 7, 11, 12], noise=0.2)\n",
        "discount = 0.95\n",
        "tolerance = 0.1"
      ],
      "metadata": {
        "id": "itlc8pqKiacY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**trail 1**\n",
        "we are using a discount factor of 0.95, a tolerance of 0.1, and the model will operate without any noise."
      ],
      "metadata": {
        "id": "8t3cZd88MPCM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset and solve using value iteration\n",
        "simple_gw.reset()\n",
        "print(\"Solving with value iteration (discount_factor=0.95, noise=0.0):\")\n",
        "iterations = value_iteration(simple_gw, discount, tolerance=0.1)\n",
        "print(simple_gw)\n",
        "print(\"-\"*30)\n",
        "print(f\"Solution found after {iterations} trials.\\n\")\n",
        "print(\"-\"*30)\n",
        "print_policy(simple_gw,discount)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljcsVQgCYexb",
        "outputId": "75149e44-eec8-44d5-cf56-921d8f694bed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with value iteration (discount_factor=0.95, noise=0.0):\n",
            "  3.15   2.99  -5.00   4.51   4.75 \n",
            "  3.32   ----   ----   4.75   5.00 \n",
            "  3.49   ----   ----   5.00   GOAL \n",
            "  3.68   3.87   4.07  -5.00   5.00 \n",
            "  3.49  -5.00   4.29   4.51   4.75 \n",
            "\n",
            "------------------------------\n",
            "Solution found after 12 trials.\n",
            "\n",
            "------------------------------\n",
            " ↓   ←  ☠️  →   ↓ \n",
            " ↓  🚫 🚫  →   ↓ \n",
            " ↓  🚫 🚫  →  🥳\n",
            " →   →   ↓  ☠️  ↑ \n",
            " ↑  ☠️  →   →   ↑ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**trail 2**\n",
        "we are using a discount factor of 0.75, a tolerance of 0.1, and the model will operate without any noise."
      ],
      "metadata": {
        "id": "Gr2fWJ3ZNZPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reset and solve using value iteration\n",
        "simple_gw.reset()\n",
        "print(\"Solving with value iteration (discount_factor=0.75, noise=0.0):\")\n",
        "iterations = value_iteration(simple_gw, discount_factor=0.75, tolerance=0.1)\n",
        "print(\"-\"*30)\n",
        "print(simple_gw)\n",
        "print(f\"Solution found after {iterations} trials.\\n\")\n",
        "print(\"-\"*30)\n",
        "print_policy(simple_gw,discount=0.75)"
      ],
      "metadata": {
        "id": "SDB6YgKhMrYh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3517ee05-7398-4289-96c5-26d0164e6718"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with value iteration (discount_factor=0.75, noise=0.0):\n",
            "------------------------------\n",
            "  0.38   0.28  -5.00   2.81   3.75 \n",
            "  0.50   ----   ----   3.75   5.00 \n",
            "  0.67   ----   ----   5.00   GOAL \n",
            "  0.89   1.19   1.58  -5.00   5.00 \n",
            "  0.67  -5.00   2.11   2.81   3.75 \n",
            "\n",
            "Solution found after 12 trials.\n",
            "\n",
            "------------------------------\n",
            " ↓   ←  ☠️  →   ↓ \n",
            " ↓  🚫 🚫  →   ↓ \n",
            " ↓  🚫 🚫  →  🥳\n",
            " →   →   ↓  ☠️  ↑ \n",
            " ↑  ☠️  →   →   ↑ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a noisy grid world"
      ],
      "metadata": {
        "id": "lmv4hKnaMsTy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**trail 3**\n",
        "\n",
        "we are using a discount factor of 0.95, a tolerance of 0.1, and the model will operate with some noise."
      ],
      "metadata": {
        "id": "92doVN-ENepO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Solving with value iteration (discount_factor=0.95, noise=0.2):\")\n",
        "iterations = value_iteration(noisy_gw, discount_factor=0.95, tolerance=0.1)\n",
        "print(noisy_gw)\n",
        "print(\"-\"*30)\n",
        "print(f\"Solution found after {iterations} trials.\\n\")\n",
        "print(\"-\"*30)\n",
        "print_policy(noisy_gw,discount)"
      ],
      "metadata": {
        "id": "FjQ5JmhtM_dz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a7fda8-e1a7-4a73-f7ca-81dfbe03a303"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with value iteration (discount_factor=0.95, noise=0.2):\n",
            "  0.42  -0.10  -5.00   3.60   4.51 \n",
            "  0.56   ----   ----   4.49   4.88 \n",
            "  0.65   ----   ----   4.22   GOAL \n",
            "  0.72   0.83   1.40  -5.00   4.17 \n",
            "  0.23  -5.00   2.09   2.90   3.84 \n",
            "\n",
            "------------------------------\n",
            "Solution found after 15 trials.\n",
            "\n",
            "------------------------------\n",
            " ↓   ←  ☠️  →   ↓ \n",
            " ↓  🚫 🚫  →   ↓ \n",
            " ↓  🚫 🚫  →  🥳\n",
            " →   →   ↓  ☠️  ↑ \n",
            " ↑  ☠️  →   →   ↑ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Solving with value iteration (discount_factor=0.95, noise=0.2):\")\n",
        "iterations = value_iteration(noisy_gw, discount_factor=0.95, tolerance=0.1)\n",
        "print(noisy_gw)\n",
        "print(\"-\"*30)\n",
        "print(f\"Solution found after {iterations} trials.\\n\")\n",
        "print(\"-\"*30)\n",
        "print_policy(noisy_gw,discount)\n",
        "# optimum policy"
      ],
      "metadata": {
        "id": "M4sPRq8xG6fg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b0fcf8e-0a00-4a99-9013-fd60dda9c2ea"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with value iteration (discount_factor=0.95, noise=0.2):\n",
            "  0.42  -0.10  -5.00   3.60   4.51 \n",
            "  0.56   ----   ----   4.49   4.88 \n",
            "  0.65   ----   ----   4.22   GOAL \n",
            "  0.72   0.83   1.40  -5.00   4.17 \n",
            "  0.23  -5.00   2.09   2.90   3.84 \n",
            "\n",
            "------------------------------\n",
            "Solution found after 15 trials.\n",
            "\n",
            "------------------------------\n",
            " ↓   ←  ☠️  →   ↓ \n",
            " ↓  🚫 🚫  →   ↓ \n",
            " ↓  🚫 🚫  →  🥳\n",
            " →   →   ↓  ☠️  ↑ \n",
            " ↑  ☠️  →   →   ↑ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**trail 4**\n",
        " we are using a discount factor of 0.75, a tolerance of 0.1, and the model will operate with some noise."
      ],
      "metadata": {
        "id": "fCsfN-QANt9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Solving with value iteration (discount_factor=0.75, noise=0.2):\")\n",
        "iterations = value_iteration(noisy_gw, discount_factor=0.75, tolerance=0.1)\n",
        "print(noisy_gw)\n",
        "print(f\"Solution found after {iterations} trials.\\n\")\n",
        "print(\"-\"*30)\n",
        "print_policy(noisy_gw,discount=0.75)\n",
        "# Bad Policy"
      ],
      "metadata": {
        "id": "d6tsRb6Ph1Xo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e261094c-8643-45ba-84b5-24f883ef3fca"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with value iteration (discount_factor=0.75, noise=0.2):\n",
            " -0.02  -0.39  -5.00   1.81   3.12 \n",
            " -0.00   ----   ----   3.17   4.54 \n",
            " -0.00   ----   ----   4.03   GOAL \n",
            " -0.04  -0.39  -0.08  -5.00   4.00 \n",
            " -0.40  -5.00   0.51   1.40   2.74 \n",
            "\n",
            "Solution found after 7 trials.\n",
            "\n",
            "------------------------------\n",
            " ↓   ←  ☠️  ↓   ↓ \n",
            " ←  🚫 🚫  →   ↓ \n",
            " ↑  🚫 🚫  →  🥳\n",
            " ↑   ←   ↓  ☠️  ↑ \n",
            " ↑  ☠️  →   →   ↑ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**trail 4**\n",
        "We are using a discount factor of 0.75 and a smaller tolerance of 0.00001. Additionally, the model will operate with some noise"
      ],
      "metadata": {
        "id": "rHoSpRnYOV2q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Solving with value iteration (discount_factor=0.75, noise=0.2):\")\n",
        "iterations = value_iteration(noisy_gw, discount_factor=0.75, tolerance=0.00001)\n",
        "print(noisy_gw)\n",
        "print(f\"Solution found after {iterations} trials.\\n\")\n",
        "print(\"**----------------------**\")\n",
        "print_policy(noisy_gw,discount=0.75)"
      ],
      "metadata": {
        "id": "g6x5LbZ_nVdz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a5192e9-d3fc-433b-fcd7-26894e8831a4"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Solving with value iteration (discount_factor=0.75, noise=0.2):\n",
            " -0.02  -0.39  -5.00   1.82   3.13 \n",
            " -0.00   ----   ----   3.18   4.54 \n",
            " -0.01   ----   ----   4.03   GOAL \n",
            " -0.04  -0.37  -0.03  -5.00   4.00 \n",
            " -0.40  -5.00   0.54   1.41   2.75 \n",
            "\n",
            "Solution found after 20 trials.\n",
            "\n",
            "**----------------------**\n",
            " ↓   ←  ☠️  ↓   ↓ \n",
            " →  🚫 🚫  →   ↓ \n",
            " ↑  🚫 🚫  →  🥳\n",
            " ↑   →   ↓  ☠️  ↑ \n",
            " ↑  ☠️  →   →   ↑ \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compare using: different discount factors, noise values, number of iterations, tolerance values"
      ],
      "metadata": {
        "id": "JElaoCdtJVYm"
      },
      "execution_count": 15,
      "outputs": []
    }
  ]
}